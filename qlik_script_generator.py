import yaml
import os
from pathlib import Path
from typing import Dict, List, Any

def generate_das_qvs(
    script_path: Path,
    schema_path: Path
) -> None:
    """
    Generates Qlik Sense QVS scripts based on a YAML schema file.

    This function reads a YAML schema file to generate QVS scripts for loading data
    into Qlik Sense. It processes each table in the schema, constructing scripts
    that set up variables, define hash tables, check for existing QVD targets,
    load new data, calculate hash values, and store the data into QVD files.
    The function also adds comments based on field descriptions and logs the process.

    Steps:
    1. Reads the YAML schema to extract table and column definitions.
    2. Generates QVS scripts for each table, including variable setup, hash table
       definitions, data loading with incremental values, and storage operations.
    3. Writes the constructed QVS scripts to an output file.

    Output:
        Generates a QVS script file at the defined output path.
    """

    # Paths
    output_path = script_path / "data_according_to_system.qvs"
    
    # Read the YAML schema file
    with open(schema_path, 'r') as f:
        schema_data = yaml.safe_load(f)
    
    # Begin constructing the output
    output = []
    
    # Add header trace
    output.append("Trace")
    output.append("===============================================================")
    output.append("    DATA ACCORDING TO SYSTEM")
    output.append("    Script generated by Qlik Script Generator")
    output.append("===============================================================")
    output.append(";")
    
    # Process each table in the schema
    if 'tables' in schema_data:
        for table_name, table_info in schema_data['tables'].items():
            # Extract entity name from table name for primary key detection
            entity_name = table_name.split('__')[-1]
            
            # Generate table script from scratch based on template format
            table_lines = []
            
            # Table header
            table_lines.append("Trace")
            table_lines.append("---------------------------------------------------------------")
            table_lines.append(f"    Extracting {table_name}")
            table_lines.append("---------------------------------------------------------------")
            table_lines.append(";")
            
            # Variables setup
            table_lines.append("Trace Setting variables...;")
            table_lines.append(f"Let val__source_path = 'lib://OneDrive - mattias.thalen@two.se/Qlik/Analytical Data Storage System/data/das.{table_name}.parquet';")
            table_lines.append(f"Let val__target_path = '$(val__qvd_path__das)/{table_name}.qvd';")
            table_lines.append("Let val__source_create_time = Timestamp(FileTime('$(val__source_path)'), 'YYYY-MM-DD hh:mm:ss.fff');")
            table_lines.append("Let val__target_create_time = Timestamp(FileTime('$(val__target_path)'), 'YYYY-MM-DD hh:mm:ss.fff');")
            table_lines.append("Let val__target_exists = If(Len('$(val__target_create_time)') > 0, 1, 0);")
            table_lines.append("Let val__source_is_newer = If('$(val__source_create_time)' > '$(val__target_create_time)', 1, 0);")
            table_lines.append("Let val__incremental_value = '1970-01-01';")
            table_lines.append("")
            
            # Check if source is newer than target
            table_lines.append("If $(val__source_is_newer) = 1 Then")
            table_lines.append("    Trace Source is newer, loading & transforming data...;")
            table_lines.append("")

            # Hash table definition
            table_lines.append("    Trace Define hash table...;")
            table_lines.append("    [processed_record_hashes]:")
            table_lines.append("    Load")
            table_lines.append("        Null() As [old_record_hash]")
            table_lines.append("    AutoGenerate 0")
            table_lines.append("    ;")
            table_lines.append("")
            
            # Check if target exists
            table_lines.append("    Trace Checking if target QVD exists...;")
            table_lines.append("    If $(val__target_exists) = 1 Then")
            table_lines.append("        Trace Target found, loading hashes and max incremental value...;")
            table_lines.append("")
            table_lines.append("        Concatenate([processed_record_hashes])")
            table_lines.append("        Load")
            table_lines.append("            [record_hash] As [old_record_hash]")
            table_lines.append("")
            table_lines.append("        From")
            table_lines.append(f"            [$(val__target_path)] (qvd)")
            table_lines.append("        ;")
            table_lines.append("")
            table_lines.append("        [max_incremental_value]:")
            table_lines.append("        Load")
            table_lines.append("            Date(Max(Num#([modified_date])), 'YYYY-MM-DD') As [max_incremental_value]")
            table_lines.append("        From")
            table_lines.append(f"            [$(val__target_path)] (qvd)")
            table_lines.append("        ;")
            table_lines.append("")
            table_lines.append("        Let val__incremental_value = Coalesce(Peek('max_incremental_value', -1, 'max_incremental_value'), '$(val__incremental_value)');")
            table_lines.append("        Drop Table [max_incremental_value];")
            table_lines.append("")
            table_lines.append("    Else")
            table_lines.append("        Trace Target not found, starting full load...;")
            table_lines.append("")
            table_lines.append("    End If")
            table_lines.append("")
            
            # Generate hash fields based on columns
            hash_fields = []
            field_load_lines = []
            field_comments = []
            
            if 'columns' in table_info:
                # Initialize field categories
                primary_keys = []
                foreign_keys = []
                regular_fields = []
                system_fields = []  # For rowguid and modified_date
                
                # Categorize fields
                for column_name, column_info in table_info['columns'].items():
                    if column_name.startswith('_dlt_'):
                        continue
                    elif column_name in ['rowguid', 'modified_date']:
                        system_fields.append((column_name, column_info))
                    elif column_name.endswith('_id') and entity_name in column_name:
                        # Primary key - if entity name is in the column name and it ends with _id
                        primary_keys.append((column_name, column_info))
                    elif column_name.endswith('_id'):
                        # Foreign key - any other column ending with _id
                        foreign_keys.append((column_name, column_info))
                    else:
                        # Regular fields - everything else
                        regular_fields.append((column_name, column_info))
                
                # Sort each category
                primary_keys.sort(key=lambda x: x[0])
                foreign_keys.sort(key=lambda x: x[0])
                regular_fields.sort(key=lambda x: x[0])
                system_fields.sort(key=lambda x: x[0])
                
                # Combine all fields in the desired order
                sorted_columns = primary_keys + foreign_keys + regular_fields + system_fields
                
                # Generate field lists
                for i, (column_name, column_info) in enumerate(sorted_columns):
                    # Add comma at the end of each line except the last one for hash fields
                    if i < len(sorted_columns) - 1:
                        hash_fields.append(f"        [{column_name}],")
                    else:
                        hash_fields.append(f"        [{column_name}]")
                    
                    # Same for field loading lines
                    field_load_lines.append(f"        Text([{column_name}]) As [{column_name}],")
                    
                    if 'description' in column_info:
                        # Escape single quotes in descriptions by replacing them with Chr(39)
                        desc = column_info['description']
                        desc = desc.replace("'", "$(=Chr39())")
                        field_comments.append(f"    Comment Field [{column_name}] With '{desc}';")
            
            # Load new data with hash calculation
            table_lines.append("    Trace Loading new data with incremental value $(val__incremental_value)...;")
            
            # Hash calculation
            table_lines.append("    Set var__record_hash = Hash256(")
            table_lines.extend(hash_fields)
            table_lines.append("    )")
            table_lines.append("    ;")
            table_lines.append("")
            
            # Field loading
            table_lines.append(f"    [{table_name}]:")
            table_lines.append("    Load")
            table_lines.extend(field_load_lines)
            table_lines.append("        $(var__record_hash) As [record_hash],")
            table_lines.append("        Timestamp#('$(val__utc)', 'YYYY-MM-DD hh:mm:ss.fff') As [record_loaded_at]")
            table_lines.append("")
            table_lines.append("    From")
            table_lines.append(f"        [$(val__source_path)] (parquet)")
            table_lines.append("")
            table_lines.append("    Where")
            table_lines.append("        1 = 1")
            table_lines.append("        And Date([modified_date], 'YYYY-MM-DD') >= Date#('$(val__incremental_value)', 'YYYY-MM-DD')")
            table_lines.append("        And Not Exists ([old_record_hash], $(var__record_hash))")
            table_lines.append("    ;")
            table_lines.append("")
            
            # Cleanup and counting
            table_lines.append("    Trace Dropping hash table...;")
            table_lines.append("    Drop Table [processed_record_hashes];")
            table_lines.append("")
            table_lines.append("    Trace Counting new records...;")
            table_lines.append(f"    Set val__no_of_new_records = Alt(NoOfRows('{table_name}'), 0);")
            table_lines.append("")
            table_lines.append("    Trace Checking if there are new records...;")
            table_lines.append("    If $(val__no_of_new_records) > 0 Then")
            table_lines.append("")
            table_lines.append("        Trace Checking if target QVD exists...;")
            table_lines.append("        If $(val__target_exists) = 1 Then")
            table_lines.append("            Trace Appending previously ingested data...;")
            table_lines.append("")
            table_lines.append(f"            Concatenate([{table_name}])")
            table_lines.append("            Load * From [$(val__target_path)] (qvd) Where Not Exists ([record_hash]);")
            table_lines.append("")
            table_lines.append("        Else")
            table_lines.append("            Trace Target not found, skipping append...;")
            table_lines.append("")
            table_lines.append("        End If")
            table_lines.append("")
            
            # Comments
            if 'description' in table_info:
                table_lines.append("        Trace Commenting table...;")
                table_desc = table_info['description']
                table_desc = table_desc.replace("'", "$(=Chr39())")
                table_lines.append(f"        Comment Table [{table_name}] With '{table_desc}';")
                table_lines.append("")
            
            table_lines.append("        Trace Commenting fields...;")
            if field_comments:
                for comment in field_comments:
                    table_lines.append(f"    {comment}")
                
            table_lines.append(f"        Comment Field [record_hash] With 'Hash of the record, used for deduplication.';")
            table_lines.append(f"        Comment Field [record_loaded_at] With 'Timestamp when the record was loaded.';")
            table_lines.append("")

            
            # Storing and cleanup
            table_lines.append("        Trace Storing data...;")
            table_lines.append(f"        Store [{table_name}] Into [$(val__qvd_path__das)/{table_name}.qvd] (qvd);")
            table_lines.append("")
            table_lines.append("    Else")
            table_lines.append("        Trace No new records loaded...;")
            table_lines.append("")
            table_lines.append("    End If")
            table_lines.append("")
            table_lines.append("    Trace Dropping table...;")
            table_lines.append(f"    Drop Table [{table_name}];")
            table_lines.append("")
            table_lines.append("Else")
            table_lines.append("    Trace Source is older than target, skipping...;")
            table_lines.append("")
            table_lines.append("End If")
            table_lines.append("")

            # Resetting variables
            table_lines.append("Trace Resetting variables...;")
            table_lines.append("Let val__source_path = Null();")
            table_lines.append("Let val__target_path = Null();")
            table_lines.append("Let var__source_create_time = Null();")
            table_lines.append("Let var__target_create_time = Null();")
            table_lines.append("Let val__target_exists = Null();")
            table_lines.append("Let val__source_is_newer = Null();")
            table_lines.append("Let val__incremental_value = Null();")
            table_lines.append("Let var__record_hash = Null();")
            table_lines.append("Let val__no_of_new_records = Null();")
            table_lines.append("")
            
            # Add the table script to the output
            output.extend(table_lines)
            
    # Write the output file
    with open(output_path, 'w') as f:
        f.write('\n'.join(output))
    
    print(f"Generated DAS QVS file at: {output_path}")

def generate_dab_qvs(
    script_path: Path,
    schema_path: Path,
    hooks_path: Path
) -> None:
    """
    Generates Qlik Sense QVS scripts for the Data According to Business (DAB) layer.

    This function reads a YAML schema file and hooks configuration to generate QVS scripts
    for transforming data from the Data According to System (DAS) layer into a business-oriented
    format. It creates hook variables for entity relationships, applies business rules, and
    formats field names according to business conventions.

    Steps:
    1. Reads the YAML schema and hooks configuration files.
    2. For each frame defined in the hooks configuration, generates a QVS script that:
       - Sets up variables for record versioning and validity periods.
       - Creates hook variables for entity relationships.
       - Loads and transforms data from the corresponding DAS table.
       - Applies field prefixing and comments.
       - Stores the transformed data in the DAB QVD file.
    3. Writes the constructed QVS scripts to an output file.

    Output:
        Generates a QVS script file at the defined output path.
    """

    # Paths
    output_path = script_path / "data_according_to_business.qvs"
    
    # Read the YAML schema file
    with open(schema_path, 'r') as f:
        schema_data = yaml.safe_load(f)
    
    # Read the hooks configuration file
    with open(hooks_path, 'r') as f:
        hooks_data = yaml.safe_load(f)
    
    # Begin constructing the output
    output = []
    
    # Add header trace
    output.append("Trace")
    output.append("===============================================================")
    output.append("    DATA ACCORDING TO BUSINESS")
    output.append("    Script generated by Qlik Script Generator")
    output.append("===============================================================")
    output.append(";")
    
    # Process each frame in the hooks configuration, sorted by name
    if 'frames' in hooks_data:
        # Sort frames by name
        sorted_frames = sorted(hooks_data['frames'], key=lambda x: x['name'])
        for frame in sorted_frames:
            frame_name = frame['name']
            source_table = frame['source_table']
            column_prefix = frame['column_prefix']
            hooks = frame['hooks']
            
            # Get table description from schema if available
            table_description = ""
            if 'tables' in schema_data and source_table in schema_data['tables']:
                table_info = schema_data['tables'][source_table]
                if 'description' in table_info:
                    table_description = table_info['description']
            
            # Generate frame script
            frame_lines = []
            
            # Frame header
            frame_lines.append("Trace")
            frame_lines.append("---------------------------------------------------------------")
            frame_lines.append(f"    Defining {frame_name}")
            frame_lines.append("---------------------------------------------------------------")
            frame_lines.append(";")
            
            # Add trace before setting variables
            frame_lines.append("Trace Setting variables...;")
            
            # Add variables for source and target QVD creation times
            frame_lines.append(f"Set var__source_qvd_create_time = QvdCreateTime([lib://DataFiles/Analytical Data Storage System/QVD/main/data_according_to_system/{source_table}.qvd]);")
            frame_lines.append(f"Set var__target_qvd_create_time = QvdCreateTime([$(val__qvd_path__dab)/{frame_name}.qvd]);")

            # Process hooks to generate hook variables first
            primary_hook = None
            composite_hooks = []
            
            # First pass - handle regular hooks
            for hook in hooks:
                # Skip hooks without name
                if 'name' not in hook:
                    continue
                    
                hook_name = hook['name']
                
                # Store composite hooks for later processing
                if 'composite_key' in hook:
                    composite_hooks.append(hook)
                    continue
                    
                # Skip if this is a non-composite hook that is missing required fields
                if 'composite_key' not in hook and ('keyset' not in hook or 'business_key_field' not in hook):
                    continue
                
                keyset = hook['keyset']
                business_key_field = hook['business_key_field']
                
                # Generate hook variable with the full hook name including prefix
                frame_lines.append(f"Set var_{hook_name} = '{keyset}|' & Text([{business_key_field}]);")
                
                if hook.get('primary', False):
                    primary_hook = hook_name
            
            # Now process composite hooks (need to be done after all component hooks are processed)
            for hook in composite_hooks:
                hook_name = hook['name']
                
                if 'composite_key' in hook:
                    components = hook['composite_key']
                    # Generate the composite hook by joining all components with '~'
                    composite_parts = [f"$(var_{comp})" for comp in components]
                    composite_expression = " & '~' & ".join(composite_parts)
                    frame_lines.append(f"Set var_{hook_name} = {composite_expression};")
                    
                    if hook.get('primary', False):
                        primary_hook = hook_name
            
            # Variables setup for record versioning - get primary key fields from schema
            primary_key_fields = []
            
            # Find the primary key fields in the schema for this table
            if 'tables' in schema_data and source_table in schema_data['tables']:
                table_info = schema_data['tables'][source_table]
                if 'columns' in table_info:
                    for col_name, col_info in table_info['columns'].items():
                        # Check if this column is a primary key
                        if col_info.get('primary_key', False):
                            primary_key_fields.append(col_name)
            
            if not primary_key_fields and primary_hook:
                # If no primary keys found but we have a primary hook, use the business key field
                for hook in hooks:
                    if hook.get('primary', False) and 'business_key_field' in hook:
                        primary_key_fields.append(hook['business_key_field'])
                        break
            
            # If still no primary keys, use the ID column if it exists, or all columns as fallback
            if not primary_key_fields:
                # Try to find an ID column as a last resort
                if 'tables' in schema_data and source_table in schema_data['tables']:
                    table_info = schema_data['tables'][source_table]
                    if 'columns' in table_info:
                        # Look for columns that might be IDs
                        id_columns = [col for col in table_info['columns'].keys() 
                                      if col.endswith('_id') or col == 'id']
                        if id_columns:
                            primary_key_fields = id_columns
            
            # Create the partition by clause for the window functions
            partition_by = ', '.join([f"[{field}]" for field in primary_key_fields])
            
            # Record version variable
            frame_lines.append(f"Set var__record_version = Window(RecNo(), {partition_by}, 'Asc', [record_loaded_at]);")
            frame_lines.append("")
            
            # Valid from date variable
            frame_lines.append("Set var__valid_from = If(")
            frame_lines.append("        $(var__record_version) = 1,")
            frame_lines.append("        Timestamp#('1970-01-01 00:00:00.000', 'YYYY-MM-DD hh:mm:ss.fff'),")
            frame_lines.append("        [record_loaded_at]")
            frame_lines.append("    )")
            frame_lines.append(";")
            frame_lines.append("")
            
            # Valid to date variable
            frame_lines.append("Set var__valid_to = Coalesce(")
            frame_lines.append(f"        Window([record_loaded_at], {partition_by}, 'Asc', [record_loaded_at], 1, 1, 1),")

            frame_lines.append("        Timestamp#('9999-12-31 23:59:59.999', 'YYYY-MM-DD hh:mm:ss.fff')")
            frame_lines.append("    )")
            frame_lines.append(";")
            frame_lines.append("")
            
            # Is current record flag
            frame_lines.append("Set var__is_current_record = If(")
            frame_lines.append("        $(var__valid_to) = Timestamp#('9999-12-31 23:59:59.999', 'YYYY-MM-DD hh:mm:ss.fff'),")
            frame_lines.append("        1,")
            frame_lines.append("        0")
            frame_lines.append("    )")
            frame_lines.append(";")
            frame_lines.append("")
            
            # Record updated at variable
            frame_lines.append("Set var__record_updated_at = If(")
            frame_lines.append("        $(var__is_current_record),")
            frame_lines.append("        [record_loaded_at],")
            frame_lines.append("        $(var__valid_to)")
            frame_lines.append("    )")
            frame_lines.append(";")
            frame_lines.append("")
            
            # Generate PIT hook if primary hook exists
            if primary_hook:
                pit_var_name = f"var__pit{primary_hook}"
                hook_var_name = f"var_{primary_hook}"
                frame_lines.append(f"Set {pit_var_name} = $({hook_var_name}) & '~epoch__valid_from|' & Text($(var__valid_from));")
            
            frame_lines.append("")
            
            # Add condition to check if source QVD is newer than target QVD
            frame_lines.append("If $(var__source_qvd_create_time) > $(var__target_qvd_create_time) Or IsNull($(var__target_qvd_create_time)) Then ")
            frame_lines.append("")
            frame_lines.append("    Trace Source is newer, loading & transforming data...;")
            frame_lines.append(f"    [{frame_name}]:")
            
            # Determine fields to load from source table
            # First add hooks
            load_fields = []
            hook_fields = []

            for hook in hooks:
                # Skip hooks without name
                if 'name' not in hook:
                    continue
                    
                hook_name = hook['name']
                
                # Ensure composite hooks are processed correctly for field loading
                # Composite hooks don't need keyset/business_key_field checks
                if 'composite_key' not in hook and ('keyset' not in hook or 'business_key_field' not in hook):
                    continue
                    
                if hook.get('primary', False):
                    pit_hook_name = f"_pit{hook_name}"
                    hook_var = f"var__pit{hook_name}"
                    load_fields.append(f"        Text($({hook_var})) As [{pit_hook_name}],")
                    hook_fields.append(pit_hook_name)
                
                # For all hooks, create a regular reference
                hook_var = f"var_{hook_name}"
                load_fields.append(f"        Text($({hook_var})) As [{hook_name}],")
                hook_fields.append(hook_name)
            
            # Get columns from schema
            if 'tables' in schema_data and source_table in schema_data['tables']:
                table_info = schema_data['tables'][source_table]
                columns = table_info.get('columns', {})
                
                for col_name, col_info in columns.items():
                    # Skip internal columns
                    if col_name.startswith('_dlt'):
                        continue
                    
                    # Format column name with prefix
                    prefixed_col = f"{column_prefix}__{col_name}"
                    
                    # Determine data type and format accordingly
                    data_type = col_info.get('data_type', 'text')
                    
                    if data_type == 'bigint':
                        load_fields.append(f"        Num#([{col_name}]) As [{prefixed_col}],")
                    elif data_type == 'date':
                        load_fields.append(f"        Date(Num#([{col_name}]), 'YYYY-MM-DD') As [{prefixed_col}],")
                    elif data_type == 'double':
                        load_fields.append(f"        Num#([{col_name}]) As [{prefixed_col}],")
                    elif data_type == 'uniqueidentifier':
                        load_fields.append(f"        Text([{col_name}]) As [{prefixed_col}],")
                    else:  # Default to text
                        load_fields.append(f"        Text([{col_name}]) As [{prefixed_col}],")
            
            # Add SCD fields
            load_fields.append(f"        Text([record_hash]) As [{column_prefix}__record_hash],")
            load_fields.append(f"        Timestamp([record_loaded_at], 'YYYY-MM-DD hh:mm:ss.fff') As [{column_prefix}__record_loaded_at],")
            load_fields.append(f"        Timestamp($(var__record_updated_at), 'YYYY-MM-DD hh:mm:ss.fff') As [{column_prefix}__record_updated_at],")
            load_fields.append(f"        Num($(var__record_version)) As [{column_prefix}__record_version],")
            load_fields.append(f"        Timestamp($(var__valid_from), 'YYYY-MM-DD hh:mm:ss.fff') As [{column_prefix}__record_valid_from],")
            load_fields.append(f"        Timestamp($(var__valid_to), 'YYYY-MM-DD hh:mm:ss.fff') As [{column_prefix}__record_valid_to],")
            load_fields.append(f"        Num($(var__is_current_record)) As [{column_prefix}__is_current_record]")
            
            # Join fields and add to frame
            frame_lines.append("    Load")
            frame_lines.extend(load_fields)
            frame_lines.append("")
            frame_lines.append("    From")
            frame_lines.append(f"        [lib://DataFiles/Analytical Data Storage System/QVD/main/data_according_to_system/{source_table}.qvd] (qvd)")
            frame_lines.append("    ;")
            frame_lines.append("")
            
            # Add table comment
            if table_description:
                frame_lines.append("    Trace Commenting table...;")
                table_desc = table_description.replace("'", "$(=Chr39())")
                frame_lines.append(f"    Comment Table [{frame_name}] With '{table_desc}';")
                frame_lines.append("")
            
            # Add field comments
            field_comments = []
            
            # Hook field comments
            frame_lines.append("    Trace Commenting fields...;")
            for hook in hooks:
                # Skip hooks without name
                if 'name' not in hook:
                    continue
                    
                hook_name = hook['name']
                
                # Ensure composite hooks are processed correctly for field loading
                # Composite hooks don't need keyset/business_key_field checks
                if 'composite_key' not in hook and ('keyset' not in hook or 'business_key_field' not in hook):
                    continue
                    
                if hook.get('primary', False):
                    # Get proper naming format for PIT hook field: _pit_hook__entity
                    pit_hook_name = f"_pit{hook_name}"
                    if pit_hook_name in hook_fields:
                        frame_lines.append(f"    Comment Field [{pit_hook_name}] With 'Point in time version of {hook_name}.';")
                
                # Add different comments for composite vs regular hooks
                if 'composite_key' in hook:
                    components = hook['composite_key']
                    component_str = ' and '.join(components)
                    frame_lines.append(f"    Comment Field [{hook_name}] With 'Composite hook using {component_str}.';")
                else:
                    keyset = hook['keyset']
                    frame_lines.append(f"    Comment Field [{hook_name}] With 'Hook for {hook['business_key_field']} using keyset: {keyset}.';")
            
            # Regular field comments
            if 'tables' in schema_data and source_table in schema_data['tables']:
                table_info = schema_data['tables'][source_table]
                columns = table_info.get('columns', {})
                
                for col_name, col_info in columns.items():
                    # Skip internal columns
                    if col_name.startswith('_dlt'):
                        continue
                    
                    if 'description' in col_info:
                        prefixed_col = f"{column_prefix}__{col_name}"
                        col_desc = col_info['description'].replace("'", "$(=Chr39())")
                        frame_lines.append(f"    Comment Field [{prefixed_col}] With '{col_desc}';")
            
            # Comment SCD fields
            frame_lines.append(f"    Comment Field [{column_prefix}__record_hash] With 'Hash of the record.';")
            frame_lines.append(f"    Comment Field [{column_prefix}__record_loaded_at] With 'Date and time the record was loaded.';")
            frame_lines.append(f"    Comment Field [{column_prefix}__record_updated_at] With 'Date and time the record was last updated.';")
            frame_lines.append(f"    Comment Field [{column_prefix}__record_version] With 'Version of the record.';")
            frame_lines.append(f"    Comment Field [{column_prefix}__record_valid_from] With 'Date and time the record was valid from.';")
            frame_lines.append(f"    Comment Field [{column_prefix}__record_valid_to] With 'Date and time the record was valid to.';")
            frame_lines.append(f"    Comment Field [{column_prefix}__is_current_record] With '1 if the record is current, 0 otherwise.';")
            frame_lines.append("")
            
            # Store the data
            frame_lines.append("    Trace Storing data...;")
            frame_lines.append(f"    Store [{frame_name}] Into [$(val__qvd_path__dab)/{frame_name}.qvd] (qvd);")
            frame_lines.append("")
            
            # Drop table
            frame_lines.append("    Trace Dropping table...;")
            frame_lines.append(f"    Drop Table [{frame_name}];")
            frame_lines.append("")
            frame_lines.append("Else")
            frame_lines.append("    Trace Source QVD has not been updated since last load, skipping...;")
            frame_lines.append("")
            frame_lines.append("End If")
            frame_lines.append("")
            
            # Reset variables
            frame_lines.append("Trace Resetting variables...;")
            frame_lines.append("Let var__record_version = Null();")
            frame_lines.append("Let var__valid_from = Null();")
            frame_lines.append("Let var__valid_to = Null();")
            frame_lines.append("Let var__is_current_record = Null();")
            frame_lines.append("Let var__record_updated_at = Null();")
            
            # Reset hook variables
            for hook in hooks:
                # Skip hooks without name
                if 'name' not in hook:
                    continue
                    
                hook_name = hook['name']
                
                # All hooks should be reset, including composite hooks
                var_name = f"var_{hook_name}"
                frame_lines.append(f"Let {var_name} = Null();")
            
            # Reset PIT hook variable if it exists
            if primary_hook:
                frame_lines.append(f"Let var__pit{primary_hook} = Null();")
                
            frame_lines.append("")
            
            # Add the frame script to the output
            output.extend(frame_lines)
    
    # Write the output file
    with open(output_path, 'w') as f:
        f.write('\n'.join(output))
    
    print(f"Generated DAB QVS file at: {output_path}")

if __name__ == "__main__":
    BASE_DIR = Path(os.path.dirname(os.path.abspath(__file__)))

    schema_path = BASE_DIR / "schemas" / "raw_schema.yaml"
    hooks_path = BASE_DIR / "schemas" / "hook__frames.yml"
    script_path = BASE_DIR / "scripts"

    # Generate Data According to System (DAS) scripts
    generate_das_qvs(
        script_path=script_path,
        schema_path=schema_path
    )
    
    # Generate Data According to Business (DAB) scripts
    generate_dab_qvs(
        script_path=script_path,
        schema_path=schema_path,
        hooks_path=hooks_path
    )
